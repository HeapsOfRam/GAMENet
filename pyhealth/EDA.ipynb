{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ef91521",
   "metadata": {},
   "source": [
    "# PyHealth GAMENet Reproduction Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789ef0fa",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "First, import python libraries, pyhealth, and related pyhealth libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef8a7ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ram/code/school/dl4h/final/submit/GAMENet/pyhealth/.v2/lib/python3.8/site-packages/pyhealth/trainer.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    }
   ],
   "source": [
    "# import python libraries\n",
    "import argparse\n",
    "import sys\n",
    "import pandas\n",
    "import json\n",
    "import math\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "# import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import pyhealth libraries\n",
    "import pyhealth\n",
    "from pyhealth.datasets import MIMIC4Dataset, MIMIC3Dataset\n",
    "from pyhealth.tasks import drug_recommendation_mimic4_fn, drug_recommendation_mimic3_fn\n",
    "# import dataloader related functions\n",
    "from pyhealth.datasets.splitter import split_by_patient\n",
    "from pyhealth.datasets import split_by_patient, get_dataloader\n",
    "# import gamenet model\n",
    "from pyhealth.models import BaseModel, GAMENetLayer, RETAIN, GAMENet\n",
    "# import trainer\n",
    "from pyhealth.trainer import Trainer\n",
    "from pyhealth.data import Patient, Visit\n",
    "\n",
    "from pyhealth.datasets import SampleDataset\n",
    "from pyhealth.medcode import ATC\n",
    "from pyhealth.models.gamenet import GCN, GCNLayer\n",
    "from pyhealth.models.utils import get_last_visit, batch_to_multihot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fa3ba5",
   "metadata": {},
   "source": [
    "Next, import our custom libraries we've designed for this study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5917b32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our custom wrapper classes\n",
    "from model import ModelWrapper\n",
    "from mimic import MIMIC4, MIMICWrapper\n",
    "\n",
    "# import our constants\n",
    "from constants import (\n",
    "    #DEV,\n",
    "    #EPOCHS, LR, DECAY_WEIGHT,\n",
    "    DRUG_REC_TN, NO_HIST_TN, NO_PROC_TN,#ALL_TASKS,\n",
    "    GN_KEY, RT_KEY,\n",
    "    #MODEL_TYPES_PER_TASK, RETAIN_FEATS_PER_TASK,\n",
    "    GAMENET_EXP, RETAIN_EXP,\n",
    "    SCORE_KEY, DPV_KEY, DDI_RATE_KEY,\n",
    "    BASE_DDI_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e1e5db",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "Next, we want to set up some constants, such as the hyperparameters we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30c547d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whether to read in \"dev\" mode or not\n",
    "DEV = True\n",
    "EPOCHS = 5\n",
    "LR = 1e-3\n",
    "DECAY_WEIGHT=1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9f8cbd",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "GAMENet is implemented in [the `pyhealth` library](https://pyhealth.readthedocs.io/en/latest/api/models/pyhealth.models.GAMENet.html).\n",
    "RETAIN is also implemented this way.\n",
    "This makes it simple for me to try to reproduce parts of the original paper.\n",
    "I implemented GAMENet and RETAIN, as well as some variants of GAMENet to support different data processing tasks.\n",
    "Specifically, I performed two ablations.\n",
    "\n",
    "The first ablation was to remove the patient's history information.\n",
    "The drug preparation task did not build a history of a patient's procedures, conditions, or prescriptions.\n",
    "Next, within the GAMENet model I removed the Dynamic Memory component present with the GAMENet layer.\n",
    "This removed any possibility of using a patient's prior drugs to make recommendations.\n",
    "\n",
    "The second ablation removed patient procedure information.\n",
    "This involved simply omitting procedure information in the data preparation task.\n",
    "Then, I modified GAMENet so that it did not intake procedures in its `forward` function.\n",
    "\n",
    "### History Ablation\n",
    "\n",
    "#### No Hist Drug Recommendation Task\n",
    "\n",
    "First, we prepare the data without accounting for patient history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0015e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drug_recommendation_mimic4_no_hist(patient: Patient):\n",
    "    samples = []\n",
    "    for i in range(len(patient)):\n",
    "        visit: Visit = patient[i]\n",
    "        conditions = visit.get_code_list(table=\"diagnoses_icd\")\n",
    "        procedures = visit.get_code_list(table=\"procedures_icd\")\n",
    "        drugs = visit.get_code_list(table=\"prescriptions\")\n",
    "        # ATC 3 level\n",
    "        drugs = [drug[:4] for drug in drugs]\n",
    "        # exclude: visits without condition, procedure, or drug code\n",
    "        if len(conditions) * len(procedures) * len(drugs) == 0:\n",
    "            continue\n",
    "        # TODO: should also exclude visit with age < 18\n",
    "        samples.append(\n",
    "            {\n",
    "                \"visit_id\": visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                \"conditions\": conditions,\n",
    "                \"procedures\": procedures,\n",
    "                \"drugs\": drugs,\n",
    "                \"drugs_all\": drugs,\n",
    "            }\n",
    "        )\n",
    "    # exclude: patients with less than 2 visit\n",
    "    if len(samples) < 2:\n",
    "        return []\n",
    "    # dont add history, just make lists\n",
    "    samples[0][\"conditions\"] = [samples[0][\"conditions\"]]\n",
    "    samples[0][\"procedures\"] = [samples[0][\"procedures\"]]\n",
    "    samples[0][\"drugs_all\"] = [samples[0][\"drugs_all\"]]\n",
    "\n",
    "    for i in range(1, len(samples)):\n",
    "        samples[i][\"conditions\"] = [samples[i][\"conditions\"]]\n",
    "        samples[i][\"procedures\"] = [samples[i][\"procedures\"]]\n",
    "        samples[i][\"drugs_all\"] = [samples[i][\"drugs_all\"]]\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac841df",
   "metadata": {},
   "source": [
    "#### GAMENet without Hist\n",
    "\n",
    "The RETAIN model did not need to be modified to handle this ablation.\n",
    "GAMENet, however, did.\n",
    "I needed to define a variant of the GAMENetLayer to remove the Dynamic Memory component.\n",
    "Then, I made a variant of the GAMENet model to use that new GAMENetLayer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "953476f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAMENet Layer without DM component\n",
    "class GAMENetLayerNoDM(GAMENetLayer):\n",
    "    \"\"\"GAMENet layer.\n",
    "    Paper: Junyuan Shang et al. GAMENet: Graph Augmented MEmory Networks for\n",
    "    Recommending Medication Combination AAAI 2019.\n",
    "    This layer is used in the GAMENet model. But it can also be used as a\n",
    "    standalone layer.\n",
    "    Args:\n",
    "        hidden_size: hidden feature size.\n",
    "        ehr_adj: an adjacency tensor of shape [num_drugs, num_drugs].\n",
    "        ddi_adj: an adjacency tensor of shape [num_drugs, num_drugs].\n",
    "        dropout : the dropout rate. Default is 0.5.\n",
    "    Examples:\n",
    "        >>> from pyhealth.models import GAMENetLayer\n",
    "        >>> queries = torch.randn(3, 5, 32) # [patient, visit, hidden_size]\n",
    "        >>> prev_drugs = torch.randint(0, 2, (3, 4, 50)).float()\n",
    "        >>> curr_drugs = torch.randint(0, 2, (3, 50)).float()\n",
    "        >>> ehr_adj = torch.randint(0, 2, (50, 50)).float()\n",
    "        >>> ddi_adj = torch.randint(0, 2, (50, 50)).float()\n",
    "        >>> layer = GAMENetLayer(32, ehr_adj, ddi_adj)\n",
    "        >>> loss, y_prob = layer(queries, prev_drugs, curr_drugs)\n",
    "        >>> loss.shape\n",
    "        torch.Size([])\n",
    "        >>> y_prob.shape\n",
    "        torch.Size([3, 50])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        ehr_adj: torch.tensor,\n",
    "        ddi_adj: torch.tensor,\n",
    "        dropout: float = 0.5,\n",
    "    ):\n",
    "        super(GAMENetLayer, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ehr_adj = ehr_adj\n",
    "        self.ddi_adj = ddi_adj\n",
    "\n",
    "        num_labels = ehr_adj.shape[0]\n",
    "        self.ehr_gcn = GCN(adj=ehr_adj, hidden_size=hidden_size, dropout=dropout)\n",
    "        self.ddi_gcn = GCN(adj=ddi_adj, hidden_size=hidden_size, dropout=dropout)\n",
    "        self.beta = nn.Parameter(torch.FloatTensor(1))\n",
    "        self.fc = nn.Linear(2 * hidden_size, num_labels)\n",
    "        self.bce_loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        queries: torch.tensor,\n",
    "        curr_drugs: torch.tensor,\n",
    "        mask: Optional[torch.tensor] = None,\n",
    "    ) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        \"\"\"Forward propagation.\n",
    "        Args:\n",
    "            queries: query tensor of shape [patient, visit, hidden_size].\n",
    "            prev_drugs: multihot tensor indicating drug usage in all previous\n",
    "                visits of shape [patient, visit - 1, num_drugs].\n",
    "            curr_drugs: multihot tensor indicating drug usage in the current\n",
    "                visit of shape [patient, num_drugs].\n",
    "            mask: an optional mask tensor of shape [patient, visit] where 1\n",
    "                indicates valid visits and 0 indicates invalid visits.\n",
    "        Returns:\n",
    "            loss: a scalar tensor representing the loss.\n",
    "            y_prob: a tensor of shape [patient, num_labels] representing\n",
    "                the probability of each drug.\n",
    "        \"\"\"\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(queries[:, :, 0])\n",
    "\n",
    "        \"\"\"I: Input memory representation\"\"\"\n",
    "        query = get_last_visit(queries, mask)\n",
    "\n",
    "        \"\"\"G: Generalization\"\"\"\n",
    "        # memory bank\n",
    "        MB = self.ehr_gcn() - self.ddi_gcn() * torch.sigmoid(self.beta)\n",
    "\n",
    "        \"\"\"O: Output memory representation\"\"\"\n",
    "        a_c = torch.softmax(torch.mm(query, MB.t()), dim=-1)\n",
    "        o_b = torch.mm(a_c, MB)\n",
    "\n",
    "        \"\"\"R: Response\"\"\"\n",
    "        memory_output = torch.cat([query, o_b], dim=-1)\n",
    "        logits = self.fc(memory_output)\n",
    "\n",
    "        loss = self.bce_loss_fn(logits, curr_drugs)\n",
    "        y_prob = torch.sigmoid(logits)\n",
    "\n",
    "        return loss, y_prob\n",
    "\n",
    "# GAMENet Model with the custom layer without DM\n",
    "class GAMENetNoHist(GAMENet):\n",
    "    \"\"\"GAMENet model.\n",
    "    Paper: Junyuan Shang et al. GAMENet: Graph Augmented MEmory Networks for\n",
    "    Recommending Medication Combination AAAI 2019.\n",
    "    Note:\n",
    "        This model is only for medication prediction which takes conditions\n",
    "        and procedures as feature_keys, and drugs_all as label_key (i.e., both\n",
    "        current and previous drugs). It only operates on the visit level.\n",
    "    Note:\n",
    "        This model only accepts ATC level 3 as medication codes.\n",
    "    Args:\n",
    "        dataset: the dataset to train the model. It is used to query certain\n",
    "            information such as the set of all tokens.\n",
    "        embedding_dim: the embedding dimension. Default is 128.\n",
    "        hidden_dim: the hidden dimension. Default is 128.\n",
    "        num_layers: the number of layers used in RNN. Default is 1.\n",
    "        dropout: the dropout rate. Default is 0.5.\n",
    "        **kwargs: other parameters for the GAMENet layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: SampleDataset,\n",
    "        embedding_dim: int = 128,\n",
    "        hidden_dim: int = 128,\n",
    "        num_layers: int = 1,\n",
    "        dropout: float = 0.5,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(GAMENet, self).__init__(\n",
    "            dataset=dataset,\n",
    "            feature_keys=[\"conditions\", \"procedures\"],\n",
    "            label_key=\"drugs_all\",\n",
    "            mode=\"multilabel\",\n",
    "        )\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.feat_tokenizers = self.get_feature_tokenizers()\n",
    "        self.label_tokenizer = self.get_label_tokenizer()\n",
    "        self.embeddings = self.get_embedding_layers(self.feat_tokenizers, embedding_dim)\n",
    "\n",
    "        ehr_adj = self.generate_ehr_adj()\n",
    "        ddi_adj = self.generate_ddi_adj()\n",
    "\n",
    "        self.cond_rnn = nn.GRU(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.proc_rnn = nn.GRU(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.query = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "        )\n",
    "\n",
    "        # validate kwargs for GAMENet layer\n",
    "        if \"hidden_size\" in kwargs:\n",
    "            raise ValueError(\"hidden_size is determined by hidden_dim\")\n",
    "        if \"ehr_adj\" in kwargs:\n",
    "            raise ValueError(\"ehr_adj is determined by the dataset\")\n",
    "        if \"ddi_adj\" in kwargs:\n",
    "            raise ValueError(\"ddi_adj is determined by the dataset\")\n",
    "        self.gamenet = GAMENetLayerNoDM(\n",
    "            hidden_size=hidden_dim,\n",
    "            ehr_adj=ehr_adj,\n",
    "            ddi_adj=ddi_adj,\n",
    "            dropout=dropout,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def generate_ehr_adj(self) -> torch.tensor:\n",
    "        \"\"\"Generates the EHR graph adjacency matrix.\"\"\"\n",
    "        label_size = self.label_tokenizer.get_vocabulary_size()\n",
    "        ehr_adj = torch.zeros((label_size, label_size))\n",
    "        for sample in self.dataset:\n",
    "            curr_drugs = sample[\"drugs_all\"][-1]\n",
    "            encoded_drugs = self.label_tokenizer.convert_tokens_to_indices(curr_drugs)\n",
    "            for idx1, med1 in enumerate(encoded_drugs):\n",
    "                for idx2, med2 in enumerate(encoded_drugs):\n",
    "                    if idx1 >= idx2:\n",
    "                        continue\n",
    "                    ehr_adj[med1, med2] = 1\n",
    "                    ehr_adj[med2, med1] = 1\n",
    "        return ehr_adj\n",
    "\n",
    "    def generate_ddi_adj(self) -> torch.tensor:\n",
    "        \"\"\"Generates the DDI graph adjacency matrix.\"\"\"\n",
    "        atc = ATC()\n",
    "        ddi = atc.get_ddi(gamenet_ddi=True)\n",
    "        label_size = self.label_tokenizer.get_vocabulary_size()\n",
    "        vocab_to_index = self.label_tokenizer.vocabulary\n",
    "        ddi_adj = torch.zeros((label_size, label_size))\n",
    "        ddi_atc3 = [\n",
    "            [ATC.convert(l[0], level=3), ATC.convert(l[1], level=3)] for l in ddi\n",
    "        ]\n",
    "        for atc_i, atc_j in ddi_atc3:\n",
    "            if atc_i in vocab_to_index and atc_j in vocab_to_index:\n",
    "                ddi_adj[vocab_to_index(atc_i), vocab_to_index(atc_j)] = 1\n",
    "                ddi_adj[vocab_to_index(atc_j), vocab_to_index(atc_i)] = 1\n",
    "        return ddi_adj\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        conditions: List[List[List[str]]],\n",
    "        procedures: List[List[List[str]]],\n",
    "        drugs_all: List[List[List[str]]],\n",
    "        **kwargs\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Forward propagation.\n",
    "        Args:\n",
    "            conditions: a nested list in three levels [patient, visit, condition].\n",
    "            procedures: a nested list in three levels [patient, visit, procedure].\n",
    "            drugs_all: a nested list in three levels [patient, visit, drug].\n",
    "        Returns:\n",
    "            A dictionary with the following keys:\n",
    "                loss: a scalar tensor representing the loss.\n",
    "                y_prob: a tensor of shape [patient, visit, num_labels] representing\n",
    "                    the probability of each drug.\n",
    "                y_true: a tensor of shape [patient, visit, num_labels] representing\n",
    "                    the ground truth of each drug.\n",
    "        \"\"\"\n",
    "        conditions = self.feat_tokenizers[\"conditions\"].batch_encode_3d(conditions)\n",
    "        # (patient, visit, code)\n",
    "        conditions = torch.tensor(conditions, dtype=torch.long, device=self.device)\n",
    "        # (patient, visit, code, embedding_dim)\n",
    "        conditions = self.embeddings[\"conditions\"](conditions)\n",
    "        # (patient, visit, embedding_dim)\n",
    "        conditions = torch.sum(conditions, dim=2)\n",
    "        # (batch, visit, hidden_size)\n",
    "        conditions, _ = self.cond_rnn(conditions)\n",
    "\n",
    "        procedures = self.feat_tokenizers[\"procedures\"].batch_encode_3d(procedures)\n",
    "        # (patient, visit, code)\n",
    "        procedures = torch.tensor(procedures, dtype=torch.long, device=self.device)\n",
    "        # (patient, visit, code, embedding_dim)\n",
    "        procedures = self.embeddings[\"procedures\"](procedures)\n",
    "        # (patient, visit, embedding_dim)\n",
    "        procedures = torch.sum(procedures, dim=2)\n",
    "        # (batch, visit, hidden_size)\n",
    "        procedures, _ = self.proc_rnn(procedures)\n",
    "\n",
    "        # (batch, visit, 2 * hidden_size)\n",
    "        patient_representations = torch.cat([conditions, procedures], dim=-1)\n",
    "        # (batch, visit, hidden_size)\n",
    "        queries = self.query(patient_representations)\n",
    "\n",
    "        label_size = self.label_tokenizer.get_vocabulary_size()\n",
    "        drugs_all = self.label_tokenizer.batch_encode_3d(\n",
    "            drugs_all, padding=(False, False), truncation=(True, False)\n",
    "        )\n",
    "\n",
    "        curr_drugs = [p[-1] for p in drugs_all]\n",
    "        curr_drugs = batch_to_multihot(curr_drugs, label_size)\n",
    "        curr_drugs = curr_drugs.to(self.device)\n",
    "\n",
    "        # get mask\n",
    "        mask = torch.sum(conditions, dim=2) != 0\n",
    "\n",
    "        # process drugs\n",
    "        loss, y_prob = self.gamenet(queries, curr_drugs, mask)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"y_prob\": y_prob,\n",
    "            \"y_true\": curr_drugs,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4c0c83",
   "metadata": {},
   "source": [
    "## Procedure Ablation\n",
    "\n",
    "The GAMENet model also needed a few changes for this ablation.\n",
    "The only change on the RETAIN side is omitting the procedures feature from the model instantiation.\n",
    "\n",
    "#### No Proc Drug Recommendation Task\n",
    "\n",
    "First, we prepare the data without collecting patient procedure information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c6b1fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drug_recommendation_mimic4_no_proc(patient: Patient):\n",
    "    samples = []\n",
    "    for i in range(len(patient)):\n",
    "        visit: Visit = patient[i]\n",
    "        conditions = visit.get_code_list(table=\"diagnoses_icd\")\n",
    "        drugs = visit.get_code_list(table=\"prescriptions\")\n",
    "        # ATC 3 level\n",
    "        drugs = [drug[:4] for drug in drugs]\n",
    "        # exclude: visits without condition, procedure, or drug code\n",
    "        if len(conditions) * len(drugs) == 0:\n",
    "            continue\n",
    "        # TODO: should also exclude visit with age < 18\n",
    "        samples.append(\n",
    "            {\n",
    "                \"visit_id\": visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                \"conditions\": conditions,\n",
    "                \"drugs\": drugs,\n",
    "                \"drugs_all\": drugs,\n",
    "            }\n",
    "        )\n",
    "    # exclude: patients with less than 2 visit\n",
    "    if len(samples) < 2:\n",
    "        return []\n",
    "    # add history\n",
    "    samples[0][\"conditions\"] = [samples[0][\"conditions\"]]\n",
    "    samples[0][\"drugs_all\"] = [samples[0][\"drugs_all\"]]\n",
    "\n",
    "    for i in range(1, len(samples)):\n",
    "        samples[i][\"conditions\"] = samples[i - 1][\"conditions\"] + [\n",
    "            samples[i][\"conditions\"]\n",
    "        ]\n",
    "        samples[i][\"drugs_all\"] = samples[i - 1][\"drugs_all\"] + [\n",
    "            samples[i][\"drugs_all\"]\n",
    "        ]\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e17d7b2",
   "metadata": {},
   "source": [
    "#### GAMENet with No Proc\n",
    "\n",
    "Then, I need to redefine the GAMENet model to not accept procedure information in its forward function.\n",
    "I also remove any processing of procedures within this function, and remove the Gated Recurrent Unit (GRU) related to procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16dbd319",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAMENetNoProc(GAMENet):\n",
    "    \"\"\"GAMENet model.\n",
    "\n",
    "    Paper: Junyuan Shang et al. GAMENet: Graph Augmented MEmory Networks for\n",
    "    Recommending Medication Combination AAAI 2019.\n",
    "\n",
    "    Note:\n",
    "        This model is only for medication prediction which takes conditions\n",
    "        as feature_keys, and drugs_all as label_key (i.e., both\n",
    "        current and previous drugs). It only operates on the visit level.\n",
    "\n",
    "    Note:\n",
    "        This model only accepts ATC level 3 as medication codes.\n",
    "\n",
    "    Args:\n",
    "        dataset: the dataset to train the model. It is used to query certain\n",
    "            information such as the set of all tokens.\n",
    "        embedding_dim: the embedding dimension. Default is 128.\n",
    "        hidden_dim: the hidden dimension. Default is 128.\n",
    "        num_layers: the number of layers used in RNN. Default is 1.\n",
    "        dropout: the dropout rate. Default is 0.5.\n",
    "        **kwargs: other parameters for the GAMENet layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: SampleDataset,\n",
    "        embedding_dim: int = 128,\n",
    "        hidden_dim: int = 128,\n",
    "        num_layers: int = 1,\n",
    "        dropout: float = 0.5,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(GAMENet, self).__init__(\n",
    "            dataset=dataset,\n",
    "            feature_keys=[\"conditions\"],\n",
    "            label_key=\"drugs_all\",\n",
    "            mode=\"multilabel\",\n",
    "        )\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.feat_tokenizers = self.get_feature_tokenizers()\n",
    "        self.label_tokenizer = self.get_label_tokenizer()\n",
    "        self.embeddings = self.get_embedding_layers(self.feat_tokenizers, embedding_dim)\n",
    "\n",
    "        ehr_adj = self.generate_ehr_adj()\n",
    "        ddi_adj = self.generate_ddi_adj()\n",
    "\n",
    "        self.cond_rnn = nn.GRU(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.query = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "\n",
    "        # validate kwargs for GAMENet layer\n",
    "        if \"hidden_size\" in kwargs:\n",
    "            raise ValueError(\"hidden_size is determined by hidden_dim\")\n",
    "        if \"ehr_adj\" in kwargs:\n",
    "            raise ValueError(\"ehr_adj is determined by the dataset\")\n",
    "        if \"ddi_adj\" in kwargs:\n",
    "            raise ValueError(\"ddi_adj is determined by the dataset\")\n",
    "        self.gamenet = GAMENetLayer(\n",
    "            hidden_size=hidden_dim,\n",
    "            ehr_adj=ehr_adj,\n",
    "            ddi_adj=ddi_adj,\n",
    "            dropout=dropout,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def generate_ehr_adj(self) -> torch.tensor:\n",
    "        \"\"\"Generates the EHR graph adjacency matrix.\"\"\"\n",
    "        label_size = self.label_tokenizer.get_vocabulary_size()\n",
    "        ehr_adj = torch.zeros((label_size, label_size))\n",
    "        for sample in self.dataset:\n",
    "            curr_drugs = sample[\"drugs_all\"][-1]\n",
    "            encoded_drugs = self.label_tokenizer.convert_tokens_to_indices(curr_drugs)\n",
    "            for idx1, med1 in enumerate(encoded_drugs):\n",
    "                for idx2, med2 in enumerate(encoded_drugs):\n",
    "                    if idx1 >= idx2:\n",
    "                        continue\n",
    "                    ehr_adj[med1, med2] = 1\n",
    "                    ehr_adj[med2, med1] = 1\n",
    "        return ehr_adj\n",
    "\n",
    "    def generate_ddi_adj(self) -> torch.tensor:\n",
    "        \"\"\"Generates the DDI graph adjacency matrix.\"\"\"\n",
    "        atc = ATC()\n",
    "        ddi = atc.get_ddi(gamenet_ddi=True)\n",
    "        label_size = self.label_tokenizer.get_vocabulary_size()\n",
    "        vocab_to_index = self.label_tokenizer.vocabulary\n",
    "        ddi_adj = torch.zeros((label_size, label_size))\n",
    "        ddi_atc3 = [\n",
    "            [ATC.convert(l[0], level=3), ATC.convert(l[1], level=3)] for l in ddi\n",
    "        ]\n",
    "        for atc_i, atc_j in ddi_atc3:\n",
    "            if atc_i in vocab_to_index and atc_j in vocab_to_index:\n",
    "                ddi_adj[vocab_to_index(atc_i), vocab_to_index(atc_j)] = 1\n",
    "                ddi_adj[vocab_to_index(atc_j), vocab_to_index(atc_i)] = 1\n",
    "        return ddi_adj\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        conditions: List[List[List[str]]],\n",
    "        drugs_all: List[List[List[str]]],\n",
    "        **kwargs\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Forward propagation.\n",
    "\n",
    "        Args:\n",
    "            conditions: a nested list in three levels [patient, visit, condition].\n",
    "            drugs_all: a nested list in three levels [patient, visit, drug].\n",
    "\n",
    "        Returns:\n",
    "            A dictionary with the following keys:\n",
    "                loss: a scalar tensor representing the loss.\n",
    "                y_prob: a tensor of shape [patient, visit, num_labels] representing\n",
    "                    the probability of each drug.\n",
    "                y_true: a tensor of shape [patient, visit, num_labels] representing\n",
    "                    the ground truth of each drug.\n",
    "\n",
    "        \"\"\"\n",
    "        conditions = self.feat_tokenizers[\"conditions\"].batch_encode_3d(conditions)\n",
    "        # (patient, visit, code)\n",
    "        conditions = torch.tensor(conditions, dtype=torch.long, device=self.device)\n",
    "        # (patient, visit, code, embedding_dim)\n",
    "        conditions = self.embeddings[\"conditions\"](conditions)\n",
    "        # (patient, visit, embedding_dim)\n",
    "        conditions = torch.sum(conditions, dim=2)\n",
    "        # (batch, visit, hidden_size)\n",
    "        conditions, _ = self.cond_rnn(conditions)\n",
    "\n",
    "        # (batch, visit, 2 * hidden_size)\n",
    "        patient_representations = torch.cat([conditions], dim=-1)\n",
    "        # (batch, visit, hidden_size)\n",
    "        queries = self.query(patient_representations)\n",
    "\n",
    "        label_size = self.label_tokenizer.get_vocabulary_size()\n",
    "        drugs_all = self.label_tokenizer.batch_encode_3d(\n",
    "            drugs_all, padding=(False, False), truncation=(True, False)\n",
    "        )\n",
    "\n",
    "        curr_drugs = [p[-1] for p in drugs_all]\n",
    "        curr_drugs = batch_to_multihot(curr_drugs, label_size)\n",
    "        curr_drugs = curr_drugs.to(self.device)\n",
    "\n",
    "        prev_drugs = [p[:-1] for p in drugs_all]\n",
    "        max_num_visit = max([len(p) for p in prev_drugs])\n",
    "        prev_drugs = [p + [[]] * (max_num_visit - len(p)) for p in prev_drugs]\n",
    "        prev_drugs = [batch_to_multihot(p, label_size) for p in prev_drugs]\n",
    "        prev_drugs = torch.stack(prev_drugs, dim=0)\n",
    "        prev_drugs = prev_drugs.to(self.device)\n",
    "\n",
    "        # get mask\n",
    "        mask = torch.sum(conditions, dim=2) != 0\n",
    "\n",
    "        # process drugs\n",
    "        loss, y_prob = self.gamenet(queries, prev_drugs, curr_drugs, mask)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"y_prob\": y_prob,\n",
    "            \"y_true\": curr_drugs,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8b8e6d",
   "metadata": {},
   "source": [
    "## Data Tasks\n",
    "\n",
    "After the tasks and models have been created, I can use these to create a \"tasklist\" that will help us to run all the tasks we are interested in.\n",
    "I also create a dictionary showing which model variant we should use for each data preparation task.\n",
    "Finally, I create a dictionary to decide which features to use for the RETAIN model based on the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "636b4af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do some strict checking in the ModelWrapper for the allowed model types\n",
    "# so, we will actually import the gamenet variants here that we defined in the repo\n",
    "# comment this out and modify the ModelWrapper class to allow custom variants\n",
    "from alt_gamenets import GAMENetNoHist as GNNH\n",
    "from alt_gamenets import GAMENetNoProc as GNNP\n",
    "# create \"tasklist\", which is a dictionary of simple task name -> drug task\n",
    "MIMIC4_TASKS = {\n",
    "    DRUG_REC_TN: drug_recommendation_mimic4_fn,\n",
    "    NO_HIST_TN: drug_recommendation_mimic4_no_hist,\n",
    "    NO_PROC_TN: drug_recommendation_mimic4_no_proc,\n",
    "}\n",
    "# create dictionary showing what model corresponds to what task\n",
    "MODEL_TYPES_PER_TASK = {\n",
    "    DRUG_REC_TN: {GN_KEY: GAMENet, RT_KEY: RETAIN},\n",
    "    NO_HIST_TN: {GN_KEY: GNNH, RT_KEY: RETAIN},\n",
    "    NO_PROC_TN: {GN_KEY: GNNP, RT_KEY: RETAIN},\n",
    "}\n",
    "# define retain features per task\n",
    "RETAIN_DEFAULT_FEATURES = [\"conditions\", \"procedures\"]\n",
    "\n",
    "RETAIN_FEATS_PER_TASK = {\n",
    "    DRUG_REC_TN: RETAIN_DEFAULT_FEATURES,\n",
    "    NO_HIST_TN: RETAIN_DEFAULT_FEATURES,\n",
    "    NO_PROC_TN: [\"conditions\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf854b9",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We use the `MIMIC4` data class from the `mimic` import.\n",
    "Another option would be to import the `MIMIC3` data class and use that as the `dataset` below.\n",
    "For this purpose, we just use `MIMIC4` and use that to load the data and prepare it with the appropriate tasks.\n",
    "\n",
    "The data class (either `MIMIC4` or `MIMIC3` decides where the data root is.\n",
    "By default, it reads in data from `./hiddendata/extracted/{mimic3/4}/`.\n",
    "So, for MIMIC4 data the data would need to be in: `./hiddendata/extracted/mimic4/`.\n",
    "This can be changed by either modifying the MIMIC4 class directly, or modifying the data root default in the `constants` file.\n",
    "\n",
    "Finally, we use the tasks we defined above as the tasklist for the `MIMICWrapper`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a8b142c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-*-READING DEV DATA-*-\n",
      "reading mimic4 data...\n",
      "---DATA STATS FOR mimic4 DATA---\n",
      "stat\n",
      "\n",
      "Statistics of base dataset (dev=True):\n",
      "\t- Dataset: MIMIC4Dataset\n",
      "\t- Number of patients: 607\n",
      "\t- Number of visits: 1463\n",
      "\t- Number of visits per patient: 2.4102\n",
      "\t- Number of events per visit in diagnoses_icd: 11.6705\n",
      "\t- Number of events per visit in procedures_icd: 1.4846\n",
      "\t- Number of events per visit in prescriptions: 55.9850\n",
      "\n",
      "info\n",
      "\n",
      "dataset.patients: patient_id -> <Patient>\n",
      "\n",
      "<Patient>\n",
      "    - visits: visit_id -> <Visit> \n",
      "    - other patient-level info\n",
      "    \n",
      "    <Visit>\n",
      "        - event_list_dict: table_name -> List[Event]\n",
      "        - other visit-level info\n",
      "    \n",
      "        <Event>\n",
      "            - code: str\n",
      "            - other event-level info\n",
      "\n",
      "***run task: drug_recommendation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples for drug_recommendation_mimic4_fn: 100%|████████████████| 607/607 [00:00<00:00, 24046.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'visit_id': '22595853', 'patient_id': '10000032', 'conditions': [['5723', '78959', '5715', '07070', '496', '29680', '30981', 'V1582']], 'procedures': [['5491']], 'drugs': ['B01A', 'J07B', 'A12B', 'C03D', 'C03C', 'N02B', 'J05A', 'R03A', 'N07B', 'R03B'], 'drugs_all': [['B01A', 'J07B', 'A12B', 'C03D', 'C03C', 'N02B', 'J05A', 'R03A', 'N07B', 'R03B']]}\n",
      "***run task: no_hist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples for drug_recommendation_mimic4_no_hist: 100%|███████████| 607/607 [00:00<00:00, 23352.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'visit_id': '22595853', 'patient_id': '10000032', 'conditions': [['5723', '78959', '5715', '07070', '496', '29680', '30981', 'V1582']], 'procedures': [['5491']], 'drugs': ['B01A', 'J07B', 'A12B', 'C03D', 'C03C', 'N02B', 'J05A', 'R03A', 'N07B', 'R03B'], 'drugs_all': [['B01A', 'J07B', 'A12B', 'C03D', 'C03C', 'N02B', 'J05A', 'R03A', 'N07B', 'R03B']]}\n",
      "***run task: no_proc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples for drug_recommendation_mimic4_no_proc: 100%|███████████| 607/607 [00:00<00:00, 25597.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'visit_id': '22595853', 'patient_id': '10000032', 'conditions': [['5723', '78959', '5715', '07070', '496', '29680', '30981', 'V1582']], 'drugs': ['B01A', 'J07B', 'A12B', 'C03D', 'C03C', 'N02B', 'J05A', 'R03A', 'N07B', 'R03B'], 'drugs_all': [['B01A', 'J07B', 'A12B', 'C03D', 'C03C', 'N02B', 'J05A', 'R03A', 'N07B', 'R03B']]}\n"
     ]
    }
   ],
   "source": [
    "# save data in the ./hiddendata/extracted/ directory\n",
    "## this uses MIMIC4, so place data in ./hiddendata/extracted/mimic4/\n",
    "## could use MIMIC3 with from mimic import MIMIC3 and placing data in ./hiddendata/extracted/mimic3/\n",
    "dataset = MIMIC4\n",
    "# we will run all tasks possible on the dataset\n",
    "# can define custom tasklist here\n",
    "mimic = MIMICWrapper(datasource=dataset, tasks=MIMIC4_TASKS)\n",
    "mimic_data = mimic.load_data(dev=DEV)\n",
    "drug_task_data = mimic.drug_task_data()\n",
    "dataloaders = mimic.create_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddb3212c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'drug_recommendation': <pyhealth.datasets.sample_dataset.SampleDataset at 0x7f0b0bb52190>,\n",
       " 'no_hist': <pyhealth.datasets.sample_dataset.SampleDataset at 0x7f0adf0e2130>,\n",
       " 'no_proc': <pyhealth.datasets.sample_dataset.SampleDataset at 0x7f0adf0e2610>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_task_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e75da4",
   "metadata": {},
   "source": [
    "## Create DDI Matrices\n",
    "\n",
    "In order to calculate DDI Rate, we need to create the DDI matrices.\n",
    "GAMENet models have this built-in, but RETAIN does not.\n",
    "So, we craft our DDI matrices ahead of time to help us calculate the rate later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e878db1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddi_mats = {}\n",
    "\n",
    "for taskname in mimic.get_task_names():\n",
    "    model_type = MODEL_TYPES_PER_TASK[taskname][GN_KEY]\n",
    "    ddi_mats[taskname] = model_type(drug_task_data[taskname]).generate_ddi_adj()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1112d779",
   "metadata": {},
   "source": [
    "## Train the Models\n",
    "\n",
    "Previously, I defined a dictionary of taskname -> modelname -> model variant.\n",
    "Using the ModeWrapper, I can pass in the appropriate model from that dictionary to use as the base model.\n",
    "Then, I train the models for both the RETAIN baseline and for GAMENet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9858bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "retain = {}\n",
    "gamenet = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6801925c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETAIN TRAINING---\n",
      "--training retain on drug_recommendation data--\n",
      "making retain model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RETAIN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (conditions): Embedding(1908, 128, padding_idx=0)\n",
      "    (procedures): Embedding(609, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (retain): ModuleDict(\n",
      "    (conditions): RETAINLayer(\n",
      "      (dropout_layer): Dropout(p=0.5, inplace=False)\n",
      "      (alpha_gru): GRU(128, 128, batch_first=True)\n",
      "      (beta_gru): GRU(128, 128, batch_first=True)\n",
      "      (alpha_li): Linear(in_features=128, out_features=1, bias=True)\n",
      "      (beta_li): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (procedures): RETAINLayer(\n",
      "      (dropout_layer): Dropout(p=0.5, inplace=False)\n",
      "      (alpha_gru): GRU(128, 128, batch_first=True)\n",
      "      (beta_gru): GRU(128, 128, batch_first=True)\n",
      "      (alpha_li): Linear(in_features=128, out_features=1, bias=True)\n",
      "      (beta_li): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=160, bias=True)\n",
      ")\n",
      "Metrics: ['jaccard_samples', 'accuracy', 'hamming_loss', 'precision_samples', 'recall_samples', 'pr_auc_samples', 'f1_samples']\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 64\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 1e-05\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f0adf138c40>\n",
      "Monitor: accuracy\n",
      "Monitor criterion: max\n",
      "Epochs: 5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49a4956184a4e4a82051c04e8dbeaad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 / 5:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-7 ---\n",
      "loss: 0.6926\n",
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 80.38it/s]\n",
      "--- Eval epoch-0, step-7 ---\n",
      "jaccard_samples: 0.1089\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.4204\n",
      "precision_samples: 0.1350\n",
      "recall_samples: 0.4196\n",
      "pr_auc_samples: 0.1674\n",
      "f1_samples: 0.1891\n",
      "loss: 0.6852\n",
      "New best accuracy score (0.0000) at epoch-0, step-7\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5005467efbb2424cadd2ff58b3f52b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 / 5:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-1, step-14 ---\n",
      "loss: 0.6796\n",
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 83.45it/s]\n",
      "--- Eval epoch-1, step-14 ---\n",
      "jaccard_samples: 0.1234\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.3492\n",
      "precision_samples: 0.1617\n",
      "recall_samples: 0.4171\n",
      "pr_auc_samples: 0.2067\n",
      "f1_samples: 0.2115\n",
      "loss: 0.6746\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e2c83ad1af48f9b19ca38c06c5017d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 / 5:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-2, step-21 ---\n",
      "loss: 0.6597\n",
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 94.70it/s]\n",
      "--- Eval epoch-2, step-21 ---\n",
      "jaccard_samples: 0.1519\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.2691\n",
      "precision_samples: 0.2151\n",
      "recall_samples: 0.4137\n",
      "pr_auc_samples: 0.2741\n",
      "f1_samples: 0.2542\n",
      "loss: 0.6562\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e34e5be6d8742ec801d90904c287f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 / 5:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-3, step-28 ---\n",
      "loss: 0.6380\n",
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 90.67it/s]\n",
      "--- Eval epoch-3, step-28 ---\n",
      "jaccard_samples: 0.1906\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.1983\n",
      "precision_samples: 0.3131\n",
      "recall_samples: 0.4154\n",
      "pr_auc_samples: 0.3626\n",
      "f1_samples: 0.3099\n",
      "loss: 0.6221\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75128af243794b07876b117daf23ccd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 / 5:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-4, step-35 ---\n",
      "loss: 0.5921\n",
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 86.24it/s]\n",
      "--- Eval epoch-4, step-35 ---\n",
      "jaccard_samples: 0.2264\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.1544\n",
      "precision_samples: 0.4206\n",
      "recall_samples: 0.4037\n",
      "pr_auc_samples: 0.4328\n",
      "f1_samples: 0.3579\n",
      "loss: 0.5730\n",
      "Loaded best model\n",
      "RETAIN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (conditions): Embedding(1908, 128, padding_idx=0)\n",
      "    (procedures): Embedding(609, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (retain): ModuleDict(\n",
      "    (conditions): RETAINLayer(\n",
      "      (dropout_layer): Dropout(p=0.5, inplace=False)\n",
      "      (alpha_gru): GRU(128, 128, batch_first=True)\n",
      "      (beta_gru): GRU(128, 128, batch_first=True)\n",
      "      (alpha_li): Linear(in_features=128, out_features=1, bias=True)\n",
      "      (beta_li): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (procedures): RETAINLayer(\n",
      "      (dropout_layer): Dropout(p=0.5, inplace=False)\n",
      "      (alpha_gru): GRU(128, 128, batch_first=True)\n",
      "      (beta_gru): GRU(128, 128, batch_first=True)\n",
      "      (alpha_li): Linear(in_features=128, out_features=1, bias=True)\n",
      "      (beta_li): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=160, bias=True)\n",
      ")\n",
      "Metrics: ['jaccard_samples', 'accuracy', 'hamming_loss', 'precision_samples', 'recall_samples', 'pr_auc_samples', 'f1_samples']\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 64\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 1e-05\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f0adf0a93d0>\n",
      "Monitor: accuracy\n",
      "Monitor criterion: max\n",
      "Epochs: 5\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--training retain on no_hist data--\n",
      "making retain model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f1702e0b2db4c7886b1e5170657ca91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 / 5:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-7 ---\n",
      "loss: 0.7284\n",
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 76.98it/s]\n",
      "--- Eval epoch-0, step-7 ---\n",
      "jaccard_samples: 0.1183\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.4409\n",
      "precision_samples: 0.1484\n",
      "recall_samples: 0.4682\n",
      "pr_auc_samples: 0.1708\n",
      "f1_samples: 0.2021\n",
      "loss: 0.6866\n",
      "New best accuracy score (0.0000) at epoch-0, step-7\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50926d98f4984861af064f6893072b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 / 5:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-1, step-14 ---\n",
      "loss: 0.6945\n",
      "Evaluation: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 139.18it/s]\n",
      "--- Eval epoch-1, step-14 ---\n",
      "jaccard_samples: 0.1283\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.3811\n",
      "precision_samples: 0.1667\n",
      "recall_samples: 0.4596\n",
      "pr_auc_samples: 0.1999\n",
      "f1_samples: 0.2189\n",
      "loss: 0.6640\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2b2f64bbf54fdb94ab6abc0333f83b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 / 5:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-2, step-21 ---\n",
      "loss: 0.6509\n",
      "Evaluation: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 117.27it/s]\n",
      "--- Eval epoch-2, step-21 ---\n",
      "jaccard_samples: 0.1532\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.3014\n",
      "precision_samples: 0.2158\n",
      "recall_samples: 0.4651\n",
      "pr_auc_samples: 0.2557\n",
      "f1_samples: 0.2584\n",
      "loss: 0.6327\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2892471c7c234b7ea22155a1e410d598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 / 5:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-3, step-28 ---\n",
      "loss: 0.5692\n",
      "Evaluation: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 124.53it/s]\n",
      "--- Eval epoch-3, step-28 ---\n",
      "jaccard_samples: 0.1846\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.2228\n",
      "precision_samples: 0.2890\n",
      "recall_samples: 0.4424\n",
      "pr_auc_samples: 0.3279\n",
      "f1_samples: 0.3042\n",
      "loss: 0.5938\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f4de1b896049cbbf3612ca854f5927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 / 5:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-4, step-35 ---\n",
      "loss: 0.4963\n",
      "Evaluation: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 105.83it/s]\n",
      "--- Eval epoch-4, step-35 ---\n",
      "jaccard_samples: 0.2255\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.1691\n",
      "precision_samples: 0.3904\n",
      "recall_samples: 0.4240\n",
      "pr_auc_samples: 0.4038\n",
      "f1_samples: 0.3604\n",
      "loss: 0.5496\n",
      "Loaded best model\n",
      "RETAIN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (conditions): Embedding(2575, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (retain): ModuleDict(\n",
      "    (conditions): RETAINLayer(\n",
      "      (dropout_layer): Dropout(p=0.5, inplace=False)\n",
      "      (alpha_gru): GRU(128, 128, batch_first=True)\n",
      "      (beta_gru): GRU(128, 128, batch_first=True)\n",
      "      (alpha_li): Linear(in_features=128, out_features=1, bias=True)\n",
      "      (beta_li): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=165, bias=True)\n",
      ")\n",
      "Metrics: ['jaccard_samples', 'accuracy', 'hamming_loss', 'precision_samples', 'recall_samples', 'pr_auc_samples', 'f1_samples']\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 64\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 1e-05\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f0adf0a9be0>\n",
      "Monitor: accuracy\n",
      "Monitor criterion: max\n",
      "Epochs: 5\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--training retain on no_proc data--\n",
      "making retain model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe8e515b5134afda053af1e070c0dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 / 5:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-11 ---\n",
      "loss: 0.6899\n",
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 70.41it/s]\n",
      "--- Eval epoch-0, step-11 ---\n",
      "jaccard_samples: 0.1536\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.4218\n",
      "precision_samples: 0.1930\n",
      "recall_samples: 0.5045\n",
      "pr_auc_samples: 0.2205\n",
      "f1_samples: 0.2602\n",
      "loss: 0.6827\n",
      "New best accuracy score (0.0000) at epoch-0, step-11\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f935154b251846e3ad7204598bc4baaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 / 5:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-1, step-22 ---\n",
      "loss: 0.6604\n",
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 67.44it/s]\n",
      "--- Eval epoch-1, step-22 ---\n",
      "jaccard_samples: 0.1805\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.3124\n",
      "precision_samples: 0.2566\n",
      "recall_samples: 0.4479\n",
      "pr_auc_samples: 0.2907\n",
      "f1_samples: 0.2994\n",
      "loss: 0.6337\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124373a7749d49329777b0d92ddbd05c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 / 5:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-2, step-33 ---\n",
      "loss: 0.5830\n",
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 71.22it/s]\n",
      "--- Eval epoch-2, step-33 ---\n",
      "jaccard_samples: 0.2165\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.2188\n",
      "precision_samples: 0.3959\n",
      "recall_samples: 0.3908\n",
      "pr_auc_samples: 0.4033\n",
      "f1_samples: 0.3496\n",
      "loss: 0.5369\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a72a8820234d228c3dbf2449085b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 / 5:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-3, step-44 ---\n",
      "loss: 0.4903\n",
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 69.55it/s]\n",
      "--- Eval epoch-3, step-44 ---\n",
      "jaccard_samples: 0.2457\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.1672\n",
      "precision_samples: 0.5478\n",
      "recall_samples: 0.3458\n",
      "pr_auc_samples: 0.5001\n",
      "f1_samples: 0.3871\n",
      "loss: 0.4391\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76fbcfd6d5a54c25b9bec00d8e20e59e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 / 5:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-4, step-55 ---\n",
      "loss: 0.4269\n",
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 70.27it/s]\n",
      "--- Eval epoch-4, step-55 ---\n",
      "jaccard_samples: 0.2502\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.1547\n",
      "precision_samples: 0.6101\n",
      "recall_samples: 0.3302\n",
      "pr_auc_samples: 0.5532\n",
      "f1_samples: 0.3923\n",
      "loss: 0.4034\n",
      "Loaded best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GAMENET TRAINING---\n",
      "--training gamenet on drug_recommendation data--\n",
      "making gamenet model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GAMENet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (conditions): Embedding(1908, 128, padding_idx=0)\n",
      "    (procedures): Embedding(609, 128, padding_idx=0)\n",
      "  )\n",
      "  (cond_rnn): GRU(128, 128, batch_first=True)\n",
      "  (proc_rnn): GRU(128, 128, batch_first=True)\n",
      "  (query): Sequential(\n",
      "    (0): ReLU()\n",
      "    (1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (gamenet): GAMENetLayer(\n",
      "    (ehr_gcn): GCN(\n",
      "      (gcn1): GCNLayer()\n",
      "      (dropout_layer): Dropout(p=0.5, inplace=False)\n",
      "      (gcn2): GCNLayer()\n",
      "    )\n",
      "    (ddi_gcn): GCN(\n",
      "      (gcn1): GCNLayer()\n",
      "      (dropout_layer): Dropout(p=0.5, inplace=False)\n",
      "      (gcn2): GCNLayer()\n",
      "    )\n",
      "    (fc): Linear(in_features=384, out_features=160, bias=True)\n",
      "    (bce_loss_fn): BCEWithLogitsLoss()\n",
      "  )\n",
      ")\n",
      "Metrics: ['jaccard_samples', 'accuracy', 'hamming_loss', 'precision_samples', 'recall_samples', 'pr_auc_samples', 'f1_samples']\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 64\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 1e-05\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f0adf138c40>\n",
      "Monitor: accuracy\n",
      "Monitor criterion: max\n",
      "Epochs: 5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb73a50d67d443b7855e195bb3570f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 / 5:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-7 ---\n",
      "loss: 0.6223\n",
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 95.80it/s]\n",
      "--- Eval epoch-0, step-7 ---\n",
      "jaccard_samples: 0.2637\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.1107\n",
      "precision_samples: 0.6063\n",
      "recall_samples: 0.3564\n",
      "pr_auc_samples: 0.4822\n",
      "f1_samples: 0.4093\n",
      "loss: 0.5519\n",
      "New best accuracy score (0.0000) at epoch-0, step-7\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62ecc0b40944d2dba8a679abd05ca66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 / 5:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-1, step-14 ---\n",
      "loss: 0.4844\n",
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 95.43it/s]\n",
      "--- Eval epoch-1, step-14 ---\n",
      "jaccard_samples: 0.2627\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.1095\n",
      "precision_samples: 0.6344\n",
      "recall_samples: 0.3468\n",
      "pr_auc_samples: 0.5452\n",
      "f1_samples: 0.4079\n",
      "loss: 0.4558\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c64a427db64bb2a35aa317442a97f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 / 5:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-2, step-21 ---\n",
      "loss: 0.4151\n",
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 96.45it/s]\n",
      "--- Eval epoch-2, step-21 ---\n",
      "jaccard_samples: 0.2568\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.1082\n",
      "precision_samples: 0.6521\n",
      "recall_samples: 0.3301\n",
      "pr_auc_samples: 0.5589\n",
      "f1_samples: 0.4012\n",
      "loss: 0.4049\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8593c4ac424f6a8306fdd80f5374ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 / 5:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-3, step-28 ---\n",
      "loss: 0.3768\n",
      "Evaluation: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 103.72it/s]\n",
      "--- Eval epoch-3, step-28 ---\n",
      "jaccard_samples: 0.2585\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.1087\n",
      "precision_samples: 0.6424\n",
      "recall_samples: 0.3342\n",
      "pr_auc_samples: 0.5635\n",
      "f1_samples: 0.4029\n",
      "loss: 0.3532\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c4584e42ff46328c4c282391b4e2b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 / 5:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-4, step-35 ---\n",
      "loss: 0.3345\n",
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 93.89it/s]\n",
      "--- Eval epoch-4, step-35 ---\n",
      "jaccard_samples: 0.2519\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.1070\n",
      "precision_samples: 0.6855\n",
      "recall_samples: 0.3172\n",
      "pr_auc_samples: 0.5645\n",
      "f1_samples: 0.3955\n",
      "loss: 0.3226\n",
      "Loaded best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--training gamenet on no_hist data--\n",
      "making gamenet model without hist...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GAMENetNoHist(\n",
      "  (embeddings): ModuleDict(\n",
      "    (conditions): Embedding(1908, 128, padding_idx=0)\n",
      "    (procedures): Embedding(609, 128, padding_idx=0)\n",
      "  )\n",
      "  (cond_rnn): GRU(128, 128, batch_first=True)\n",
      "  (proc_rnn): GRU(128, 128, batch_first=True)\n",
      "  (query): Sequential(\n",
      "    (0): ReLU()\n",
      "    (1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (gamenet): GAMENetLayerNoDM(\n",
      "    (ehr_gcn): GCN(\n",
      "      (gcn1): GCNLayer()\n",
      "      (dropout_layer): Dropout(p=0.5, inplace=False)\n",
      "      (gcn2): GCNLayer()\n",
      "    )\n",
      "    (ddi_gcn): GCN(\n",
      "      (gcn1): GCNLayer()\n",
      "      (dropout_layer): Dropout(p=0.5, inplace=False)\n",
      "      (gcn2): GCNLayer()\n",
      "    )\n",
      "    (fc): Linear(in_features=256, out_features=160, bias=True)\n",
      "    (bce_loss_fn): BCEWithLogitsLoss()\n",
      "  )\n",
      ")\n",
      "Metrics: ['jaccard_samples', 'accuracy', 'hamming_loss', 'precision_samples', 'recall_samples', 'pr_auc_samples', 'f1_samples']\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 64\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 1e-05\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f0adf0a93d0>\n",
      "Monitor: accuracy\n",
      "Monitor criterion: max\n",
      "Epochs: 5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d821b28d369741c8b30748cc21266e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 / 5:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-7 ---\n",
      "loss: 0.6617\n",
      "Evaluation: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 257.35it/s]\n",
      "--- Eval epoch-0, step-7 ---\n",
      "jaccard_samples: 0.2252\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.1389\n",
      "precision_samples: 0.4588\n",
      "recall_samples: 0.3400\n",
      "pr_auc_samples: 0.4303\n",
      "f1_samples: 0.3588\n",
      "loss: 0.5925\n",
      "New best accuracy score (0.0000) at epoch-0, step-7\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb19df60d1a543b681b666d6e48ba444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 / 5:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-1, step-14 ---\n",
      "loss: 0.5050\n",
      "Evaluation: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 181.83it/s]\n",
      "--- Eval epoch-1, step-14 ---\n",
      "jaccard_samples: 0.2813\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.1239\n",
      "precision_samples: 0.5550\n",
      "recall_samples: 0.3908\n",
      "pr_auc_samples: 0.5154\n",
      "f1_samples: 0.4257\n",
      "loss: 0.4254\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4fba7fb20dc49c5b14a1e7a7287e3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 / 5:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-2, step-21 ---\n",
      "loss: 0.3575\n",
      "Evaluation: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 271.02it/s]\n",
      "--- Eval epoch-2, step-21 ---\n",
      "jaccard_samples: 0.3003\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.1199\n",
      "precision_samples: 0.5839\n",
      "recall_samples: 0.4146\n",
      "pr_auc_samples: 0.5557\n",
      "f1_samples: 0.4475\n",
      "loss: 0.3250\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf05defd21a94909a0cbec3e6e7de945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 / 5:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-3, step-28 ---\n",
      "loss: 0.3296\n",
      "Evaluation: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 284.78it/s]\n",
      "--- Eval epoch-3, step-28 ---\n",
      "jaccard_samples: 0.2853\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.1260\n",
      "precision_samples: 0.5362\n",
      "recall_samples: 0.4116\n",
      "pr_auc_samples: 0.5618\n",
      "f1_samples: 0.4310\n",
      "loss: 0.3134\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa947bbdb6e45baaf97756b5128f4a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 / 5:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-4, step-35 ---\n",
      "loss: 0.3127\n",
      "Evaluation: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 267.00it/s]\n",
      "--- Eval epoch-4, step-35 ---\n",
      "jaccard_samples: 0.2818\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.1279\n",
      "precision_samples: 0.5293\n",
      "recall_samples: 0.4136\n",
      "pr_auc_samples: 0.5546\n",
      "f1_samples: 0.4286\n",
      "loss: 0.3138\n",
      "Loaded best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--training gamenet on no_proc data--\n",
      "making gamenet model without procedures...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GAMENetNoProc(\n",
      "  (embeddings): ModuleDict(\n",
      "    (conditions): Embedding(2575, 128, padding_idx=0)\n",
      "  )\n",
      "  (cond_rnn): GRU(128, 128, batch_first=True)\n",
      "  (query): Sequential(\n",
      "    (0): ReLU()\n",
      "    (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (gamenet): GAMENetLayer(\n",
      "    (ehr_gcn): GCN(\n",
      "      (gcn1): GCNLayer()\n",
      "      (dropout_layer): Dropout(p=0.5, inplace=False)\n",
      "      (gcn2): GCNLayer()\n",
      "    )\n",
      "    (ddi_gcn): GCN(\n",
      "      (gcn1): GCNLayer()\n",
      "      (dropout_layer): Dropout(p=0.5, inplace=False)\n",
      "      (gcn2): GCNLayer()\n",
      "    )\n",
      "    (fc): Linear(in_features=384, out_features=165, bias=True)\n",
      "    (bce_loss_fn): BCEWithLogitsLoss()\n",
      "  )\n",
      ")\n",
      "Metrics: ['jaccard_samples', 'accuracy', 'hamming_loss', 'precision_samples', 'recall_samples', 'pr_auc_samples', 'f1_samples']\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 64\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 1e-05\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f0adf0a9be0>\n",
      "Monitor: accuracy\n",
      "Monitor criterion: max\n",
      "Epochs: 5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19b0e24baa443e49617daa250d54d98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 / 5:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-11 ---\n",
      "loss: 0.5823\n",
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 49.13it/s]\n",
      "--- Eval epoch-0, step-11 ---\n",
      "jaccard_samples: 0.2250\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.1435\n",
      "precision_samples: 0.7698\n",
      "recall_samples: 0.2546\n",
      "pr_auc_samples: 0.5918\n",
      "f1_samples: 0.3584\n",
      "loss: 0.4793\n",
      "New best accuracy score (0.0000) at epoch-0, step-11\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1764769fcad2422b844d9c7f15c79da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 / 5:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-1, step-22 ---\n",
      "loss: 0.4123\n",
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 43.98it/s]\n",
      "--- Eval epoch-1, step-22 ---\n",
      "jaccard_samples: 0.2137\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.1451\n",
      "precision_samples: 0.7670\n",
      "recall_samples: 0.2419\n",
      "pr_auc_samples: 0.6116\n",
      "f1_samples: 0.3426\n",
      "loss: 0.4697\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f06f0d9b6d4e79a586617764dd2f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 / 5:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-2, step-33 ---\n",
      "loss: 0.3305\n",
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 40.88it/s]\n",
      "--- Eval epoch-2, step-33 ---\n",
      "jaccard_samples: 0.2232\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.1438\n",
      "precision_samples: 0.7727\n",
      "recall_samples: 0.2489\n",
      "pr_auc_samples: 0.6263\n",
      "f1_samples: 0.3557\n",
      "loss: 0.4350\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446aa4391d434c12a5a42d6109e0d681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 / 5:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-3, step-44 ---\n",
      "loss: 0.2935\n",
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 50.40it/s]\n",
      "--- Eval epoch-3, step-44 ---\n",
      "jaccard_samples: 0.2659\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.1381\n",
      "precision_samples: 0.7798\n",
      "recall_samples: 0.2991\n",
      "pr_auc_samples: 0.6294\n",
      "f1_samples: 0.4088\n",
      "loss: 0.3922\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a467845d93496781057608cf786957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 / 5:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-4, step-55 ---\n",
      "loss: 0.2774\n",
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 44.33it/s]\n",
      "--- Eval epoch-4, step-55 ---\n",
      "jaccard_samples: 0.2126\n",
      "accuracy: 0.0000\n",
      "hamming_loss: 0.1440\n",
      "precision_samples: 0.7930\n",
      "recall_samples: 0.2331\n",
      "pr_auc_samples: 0.6254\n",
      "f1_samples: 0.3426\n",
      "loss: 0.3997\n",
      "Loaded best model\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "print(\"---RETAIN TRAINING---\")\n",
    "for taskname,dataloader in dataloaders.items():\n",
    "    print(\"--training retain on {} data--\".format(taskname))\n",
    "    # create and train retain model\n",
    "    retain[taskname] = ModelWrapper(\n",
    "        drug_task_data[taskname],\n",
    "        model=MODEL_TYPES_PER_TASK[taskname][RT_KEY],\n",
    "        feature_keys=RETAIN_FEATS_PER_TASK[taskname],\n",
    "        experiment=\"{}_task_{}\".format(RETAIN_EXP, taskname)\n",
    "    )\n",
    "    retain[taskname].train_model(\n",
    "        dataloader[\"train\"], dataloader[\"val\"],\n",
    "        decay_weight=DECAY_WEIGHT,\n",
    "        learning_rate=LR,\n",
    "        epochs=EPOCHS\n",
    "    )\n",
    "\n",
    "# gamenet\n",
    "print(\"---GAMENET TRAINING---\")\n",
    "for taskname,dataloader in dataloaders.items():\n",
    "    print(\"--training gamenet on {} data--\".format(taskname))\n",
    "    # create and train gamenet model\n",
    "    gamenet[taskname] = ModelWrapper(\n",
    "        drug_task_data[taskname],\n",
    "        model=MODEL_TYPES_PER_TASK[taskname][GN_KEY],\n",
    "        experiment=\"{}_task_{}\".format(GAMENET_EXP, taskname)\n",
    "    )\n",
    "    gamenet[taskname].train_model(\n",
    "        dataloader[\"train\"], dataloader[\"val\"],\n",
    "        decay_weight = DECAY_WEIGHT,\n",
    "        learning_rate = LR,\n",
    "        epochs=EPOCHS\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a4b4b1",
   "metadata": {},
   "source": [
    "## Evaluate the Models\n",
    "\n",
    "Next I evaluate all of the models for the different tasks.\n",
    "This includes getting the normal scores from `pyhealth` and also calculating DDI Rate and average drugs per visit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ce1684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_result = {}\n",
    "gamenet_result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b68dfe9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETAIN EVALUATION---\n",
      "--eval retain on drug_recommendation data--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 23.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jaccard_samples': 0.14146940962999824, 'accuracy': 0.0, 'hamming_loss': 0.42834302325581397, 'precision_samples': 0.17616063019906833, 'recall_samples': 0.4827049565056121, 'pr_auc_samples': 0.20238118914231482, 'f1_samples': 0.241459547618023, 'loss': 0.6866651773452759}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 75.87it/s]\n",
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 68.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--eval retain on no_hist data--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 104.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jaccard_samples': 0.15062685408395096, 'accuracy': 0.0, 'hamming_loss': 0.43569711538461536, 'precision_samples': 0.18602211478706557, 'recall_samples': 0.5375429973384637, 'pr_auc_samples': 0.21858402598081533, 'f1_samples': 0.25050916474858786, 'loss': 0.6869425773620605}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 91.53it/s]\n",
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 92.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--eval retain on no_proc data--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 102.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jaccard_samples': 0.12198150103349795, 'accuracy': 0.0, 'hamming_loss': 0.4001393242772553, 'precision_samples': 0.15764750921602252, 'recall_samples': 0.4667931620825103, 'pr_auc_samples': 0.1969090999965072, 'f1_samples': 0.2122508281576915, 'loss': 0.6740173697471619}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 94.05it/s]\n",
      "Evaluation: 100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 105.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GAMENET EVALUATION---\n",
      "--eval gamenet on drug_recommendation data--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 83.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jaccard_samples': 0.2782539747315031, 'accuracy': 0.0, 'hamming_loss': 0.1311046511627907, 'precision_samples': 0.7051679586563306, 'recall_samples': 0.3310068391585073, 'pr_auc_samples': 0.5450597929886727, 'f1_samples': 0.4244387699606564, 'loss': 0.5369114279747009}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 69.90it/s]\n",
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 75.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--eval gamenet on no_hist data--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 218.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jaccard_samples': 0.2577125324103029, 'accuracy': 0.0, 'hamming_loss': 0.14927884615384615, 'precision_samples': 0.5576445189425958, 'recall_samples': 0.3712612252367469, 'pr_auc_samples': 0.5113222327062721, 'f1_samples': 0.3992516398878801, 'loss': 0.5893192887306213}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 161.44it/s]\n",
      "Evaluation: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 228.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--eval gamenet on no_proc data--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 84.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jaccard_samples': 0.24298973179750794, 'accuracy': 0.0, 'hamming_loss': 0.12079414838035528, 'precision_samples': 0.6929392446633825, 'recall_samples': 0.29053999571629796, 'pr_auc_samples': 0.5123229705737585, 'f1_samples': 0.37743372050818086, 'loss': 0.4500429481267929}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 82.76it/s]\n",
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 87.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "print(\"---RETAIN EVALUATION---\")\n",
    "for taskname in mimic.get_task_names():\n",
    "    print(\"--eval retain on {} data--\".format(taskname))\n",
    "    test_loader = dataloaders[taskname][\"test\"]\n",
    "    baseline_result[taskname] = {}\n",
    "    baseline_result[taskname] = retain[taskname].evaluate_model(test_loader)\n",
    "    baseline_result[taskname][DPV_KEY] = retain[taskname].calc_avg_drugs_per_visit(test_loader)\n",
    "    baseline_result[taskname][DDI_RATE_KEY] = retain[taskname].calc_ddi_rate(test_loader, ddi_mats[taskname])\n",
    "    \n",
    "# gamenet\n",
    "print(\"---GAMENET EVALUATION---\")\n",
    "for taskname in mimic.get_task_names():\n",
    "    print(\"--eval gamenet on {} data--\".format(taskname))\n",
    "    test_loader = dataloaders[taskname][\"test\"]\n",
    "    gamenet_result[taskname] = {}\n",
    "    gamenet_result[taskname] = gamenet[taskname].evaluate_model(test_loader)\n",
    "    gamenet_result[taskname][DPV_KEY] = gamenet[taskname].calc_avg_drugs_per_visit(test_loader)\n",
    "    gamenet_result[taskname][DDI_RATE_KEY] = gamenet[taskname].calc_ddi_rate(test_loader, ddi_mats[taskname])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93edb904",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "Finally, we generate some tables and visualizations that will be used in the final report and video.\n",
    "We build these tables from the dictionaries of taskname -> model that we created during the training/evaluation step.\n",
    "Keep in mind, that if `DEV=True`, these will just be sample results and not reflective of the study.\n",
    "For an example of results run with the full dataset, [please see this old version of the notebook](https://github.com/HeapsOfRam/GAMENet/blob/5fbb96e549e5aaf9ec2e4ffeef1bff06d0589e67/pyhealth/EDA.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c1dcf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_columns = [\n",
    "    \"jaccard_samples\", \"precision_samples\", \"recall_samples\",\n",
    "    \"pr_auc_samples\", \"f1_samples\",\n",
    "    \"avg_dpv\", \"ddi_rate\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c04f0e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "retain_metrics = pandas.DataFrame.from_dict(baseline_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab8d0314",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamenet_metrics = pandas.DataFrame.from_dict(gamenet_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "269d637b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jaccard_samples</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>hamming_loss</th>\n",
       "      <th>precision_samples</th>\n",
       "      <th>recall_samples</th>\n",
       "      <th>pr_auc_samples</th>\n",
       "      <th>f1_samples</th>\n",
       "      <th>loss</th>\n",
       "      <th>avg_dpv</th>\n",
       "      <th>ddi_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>drug_recommendation</th>\n",
       "      <td>0.141469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428343</td>\n",
       "      <td>0.176161</td>\n",
       "      <td>0.482705</td>\n",
       "      <td>0.202381</td>\n",
       "      <td>0.241460</td>\n",
       "      <td>0.686665</td>\n",
       "      <td>67.372093</td>\n",
       "      <td>0.040289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_hist</th>\n",
       "      <td>0.150627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.435697</td>\n",
       "      <td>0.186022</td>\n",
       "      <td>0.537543</td>\n",
       "      <td>0.218584</td>\n",
       "      <td>0.250509</td>\n",
       "      <td>0.686943</td>\n",
       "      <td>71.307692</td>\n",
       "      <td>0.045094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_proc</th>\n",
       "      <td>0.121982</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.400139</td>\n",
       "      <td>0.157648</td>\n",
       "      <td>0.466793</td>\n",
       "      <td>0.196909</td>\n",
       "      <td>0.212251</td>\n",
       "      <td>0.674017</td>\n",
       "      <td>62.816092</td>\n",
       "      <td>0.042028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     jaccard_samples  accuracy  hamming_loss   \n",
       "drug_recommendation         0.141469       0.0      0.428343  \\\n",
       "no_hist                     0.150627       0.0      0.435697   \n",
       "no_proc                     0.121982       0.0      0.400139   \n",
       "\n",
       "                     precision_samples  recall_samples  pr_auc_samples   \n",
       "drug_recommendation           0.176161        0.482705        0.202381  \\\n",
       "no_hist                       0.186022        0.537543        0.218584   \n",
       "no_proc                       0.157648        0.466793        0.196909   \n",
       "\n",
       "                     f1_samples      loss    avg_dpv  ddi_rate  \n",
       "drug_recommendation    0.241460  0.686665  67.372093  0.040289  \n",
       "no_hist                0.250509  0.686943  71.307692  0.045094  \n",
       "no_proc                0.212251  0.674017  62.816092  0.042028  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(retain_metrics.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed7b026d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jaccard_samples</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>hamming_loss</th>\n",
       "      <th>precision_samples</th>\n",
       "      <th>recall_samples</th>\n",
       "      <th>pr_auc_samples</th>\n",
       "      <th>f1_samples</th>\n",
       "      <th>loss</th>\n",
       "      <th>avg_dpv</th>\n",
       "      <th>ddi_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>drug_recommendation</th>\n",
       "      <td>0.278254</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.131105</td>\n",
       "      <td>0.705168</td>\n",
       "      <td>0.331007</td>\n",
       "      <td>0.545060</td>\n",
       "      <td>0.424439</td>\n",
       "      <td>0.536911</td>\n",
       "      <td>9.767442</td>\n",
       "      <td>0.116531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_hist</th>\n",
       "      <td>0.257713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.149279</td>\n",
       "      <td>0.557645</td>\n",
       "      <td>0.371261</td>\n",
       "      <td>0.511322</td>\n",
       "      <td>0.399252</td>\n",
       "      <td>0.589319</td>\n",
       "      <td>12.942308</td>\n",
       "      <td>0.070128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_proc</th>\n",
       "      <td>0.242990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.120794</td>\n",
       "      <td>0.692939</td>\n",
       "      <td>0.290540</td>\n",
       "      <td>0.512323</td>\n",
       "      <td>0.377434</td>\n",
       "      <td>0.450043</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.238095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     jaccard_samples  accuracy  hamming_loss   \n",
       "drug_recommendation         0.278254       0.0      0.131105  \\\n",
       "no_hist                     0.257713       0.0      0.149279   \n",
       "no_proc                     0.242990       0.0      0.120794   \n",
       "\n",
       "                     precision_samples  recall_samples  pr_auc_samples   \n",
       "drug_recommendation           0.705168        0.331007        0.545060  \\\n",
       "no_hist                       0.557645        0.371261        0.511322   \n",
       "no_proc                       0.692939        0.290540        0.512323   \n",
       "\n",
       "                     f1_samples      loss    avg_dpv  ddi_rate  \n",
       "drug_recommendation    0.424439  0.536911   9.767442  0.116531  \n",
       "no_hist                0.399252  0.589319  12.942308  0.070128  \n",
       "no_proc                0.377434  0.450043   7.000000  0.238095  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(gamenet_metrics.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed6b691e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrrrr}\n",
      "\\toprule\n",
      " & jaccard_samples & accuracy & hamming_loss & precision_samples & recall_samples & pr_auc_samples & f1_samples & loss & avg_dpv & ddi_rate \\\\\n",
      "\\midrule\n",
      "drug_recommendation & 0.141469 & 0.000000 & 0.428343 & 0.176161 & 0.482705 & 0.202381 & 0.241460 & 0.686665 & 67.372093 & 0.040289 \\\\\n",
      "no_hist & 0.150627 & 0.000000 & 0.435697 & 0.186022 & 0.537543 & 0.218584 & 0.250509 & 0.686943 & 71.307692 & 0.045094 \\\\\n",
      "no_proc & 0.121982 & 0.000000 & 0.400139 & 0.157648 & 0.466793 & 0.196909 & 0.212251 & 0.674017 & 62.816092 & 0.042028 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(retain_metrics.T.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adb43d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrrrr}\n",
      "\\toprule\n",
      " & jaccard_samples & accuracy & hamming_loss & precision_samples & recall_samples & pr_auc_samples & f1_samples & loss & avg_dpv & ddi_rate \\\\\n",
      "\\midrule\n",
      "drug_recommendation & 0.278254 & 0.000000 & 0.131105 & 0.705168 & 0.331007 & 0.545060 & 0.424439 & 0.536911 & 9.767442 & 0.116531 \\\\\n",
      "no_hist & 0.257713 & 0.000000 & 0.149279 & 0.557645 & 0.371261 & 0.511322 & 0.399252 & 0.589319 & 12.942308 & 0.070128 \\\\\n",
      "no_proc & 0.242990 & 0.000000 & 0.120794 & 0.692939 & 0.290540 & 0.512323 & 0.377434 & 0.450043 & 7.000000 & 0.238095 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gamenet_metrics.T.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb36ff4",
   "metadata": {},
   "source": [
    "# Reproducibility Summary\n",
    "\n",
    "In the original study, GAMENet proves to be a powerful tool for recommending drug combinations to patients while avoiding adverse DDI.\n",
    "It also consistently recommends fewer drugs than other methods, especially compared to baselines.\n",
    "In general, my replication study seems to corroborate these findings.\n",
    "Through testing the GAMENet model against the RETAIN baseline and performing a couple of ablations on the original model, some interesting results were produced.\n",
    "\n",
    "Like the original paper, in my reproduction GAMEnet generated less DDI than other models.\n",
    "Unlike the original paper, in my reproduction the RETAIN baseline actually recommends fewer drugs per visit on average.\n",
    "However, this difference is only about 2 drugs per visit.\n",
    "Additionally, GAMENet maintains competitive evaluation metrics to RETAIN in many cases.\n",
    "My conclusion is that the additional 2 drugs per visit made by GAMENet is worth the tradeoff of avoiding potentially harmful DDI.\n",
    "\n",
    "The ablations proved that the procecure information is critical to making good recommendations.\n",
    "These patterns help to avoid DDI and also generally improve performance metrics.\n",
    "History is also an important characteristic to consider, however omitting these details do not hurt the model performance as much as omitting procedure information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25cb422",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "```\n",
    "@inproceedings{GAMENet:2019,\n",
    "    title=\"{GAMENet: Graph Augmented MEmory Networks for Recommending Medication Combination}\",\n",
    "    author={Junyuan Shang and Cao Xiao and Tengfei Ma and Hongyan Li and Jimeng Sun},\n",
    "    journal={arXiv preprint arXiv:1809.01852},\n",
    "    year={2019},\n",
    "    eprint={1809.01852},\n",
    "    archivePrefix={arXiv},\n",
    "    primaryClass={cs.AI}\n",
    "}\n",
    "\n",
    "@inproceedings{Doctor2Vec:2020,\n",
    "    title=\"{Doctor2Vec: Dynamic Doctor Representation Learning for Clinical Trial Recruitment}\",\n",
    "    author={Siddharth Biswal and Cao Xiao and Lucas M. Glass and Elizabeth Milkovits and Jimeng Sun},\n",
    "    year={2019},\n",
    "    eprint={1911.10395},\n",
    "    archivePrefix={arXiv},\n",
    "    primaryClass={cs.LG}\n",
    "}\n",
    "\n",
    "@inproceedings{Leap:2017,\n",
    "    author = {Zhang, Yutao and Chen, Robert and Tang, Jie and Stewart, Walter F. and Sun, Jimeng},\n",
    "    title = {LEAP: Learning to Prescribe Effective and Safe Treatment Combinations for Multimorbidity},\n",
    "    year = {2017},\n",
    "    isbn = {9781450348874},\n",
    "    publisher = {Association for Computing Machinery},\n",
    "    address = {New York, NY, USA},\n",
    "    url = {https://doi.org/10.1145/3097983.3098109},\n",
    "    doi = {10.1145/3097983.3098109},\n",
    "    abstract = {Managing patients with complex multimorbidity has long been recognized as a difficult problem due to complex disease and medication dependencies and the potential risk of adverse drug interactions. Existing work either uses complicated rule-based protocols which are hard to implement and maintain, or simple statistical models that treat each disease independently, which may lead to sub-optimal or even harmful drug combinations. In this work, we propose the LEAP (LEArn to Prescribe) algorithm to decompose the treatment recommendation into a sequential decision-making process while automatically determining the appropriate number of medications. A recurrent decoder is used to model label dependencies and content-based attention is used to capture label instance mapping. We further leverage reinforcement learning to fine tune the model parameters to ensure accuracy and completeness. We incorporate external clinical knowledge into the design of the reinforcement reward to effectively prevent generating unfavorable drug combinations. Both quantitative experiments and qualitative case studies are conducted on two real world electronic health record datasets to verify the effectiveness of our solution. On both datasets, LEAP significantly outperforms baselines by up to 10-30% in terms of mean Jaccard coefficient and removes 99.8% adverse drug interactions in the recommended treatment sets.},\n",
    "    booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},\n",
    "    pages = {1315–1324},\n",
    "    numpages = {10},\n",
    "    keywords = {multi-instance multilabel learning, multimorbidity, treatment recommendation},\n",
    "    location = {Halifax, NS, Canada},\n",
    "    series = {KDD '17}\n",
    "}\n",
    "\n",
    "@inproceedings{DMNC:2018,\n",
    "    title=\"{Dual Memory Neural Computer for Asynchronous Two-view Sequential Learning}\",\n",
    "    author={Hung Le and Truyen Tran and Svetha Venkatesh},\n",
    "    year={2018},\n",
    "    eprint={1802.00662},\n",
    "    archivePrefix={arXiv},\n",
    "    primaryClass={cs.LG}\n",
    "}\n",
    "\n",
    "@inproceedings{RETAIN:2017,\n",
    "    title=\"{RETAIN: An Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism}\",\n",
    "    author={Edward Choi and Mohammad Taha Bahadori and Joshua A. Kulas and Andy Schuetz and Walter F. Stewart and Jimeng Sun},\n",
    "    year={2017},\n",
    "    eprint={1608.05745},\n",
    "    archivePrefix={arXiv},\n",
    "    primaryClass={cs.LG}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feb795e",
   "metadata": {},
   "source": [
    "## Cite \n",
    "\n",
    "The authors have asked to cite their paper when using their work:\n",
    "\n",
    "```\n",
    "@article{shang2018gamenet,\n",
    "  title=\"{GAMENet: Graph Augmented MEmory Networks for Recommending Medication Combination}\",\n",
    "  author={Shang, Junyuan and Xiao, Cao and Ma, Tengfei and Li, Hongyan and Sun, Jimeng},\n",
    "  journal={arXiv preprint arXiv:1809.01852},\n",
    "  year={2018}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
